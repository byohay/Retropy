{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-09T13:29:26.125893Z",
     "start_time": "2018-06-09T13:29:24.938953Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"Installing requirements ..\"\n",
    "pip install pandas==0.22.0 quandl pandas_datareader alpha_vantage matplotlib plotly sklearn scipy fix_yahoo_finance statsmodels beautifulsoup4 > /dev/null 2>&1\n",
    "# NOTE: we use pandas 0.22 for now since pandas_datareader don't support 0.23 yet\n",
    "echo \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-09T13:29:28.270476Z",
     "start_time": "2018-06-09T13:29:26.128525Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numbers\n",
    "import subprocess\n",
    "import uuid\n",
    "import string\n",
    "import json \n",
    "import requests\n",
    "from io import StringIO\n",
    "import re\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import linear_model\n",
    "\n",
    "import quandl\n",
    "quandl.ApiConfig.api_key = \"9nrUn7Sm1SdoeLdQGQB-\"\n",
    "\n",
    "import pandas_datareader\n",
    "from pandas_datareader import data as pdr\n",
    "import fix_yahoo_finance as yf\n",
    "yf.pdr_override() # <== that's all it takes :-)\n",
    "import alpha_vantage\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "from alpha_vantage.cryptocurrencies import CryptoCurrencies\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 10.0) # Make plots bigger\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.graph_objs.layout as gol\n",
    "py.init_notebook_mode()\n",
    "\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-09T13:29:28.282381Z",
     "start_time": "2018-06-09T13:29:28.274102Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-09T13:29:28.291316Z",
     "start_time": "2018-06-09T13:29:28.285632Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pd_from_dict(d):\n",
    "    return pd.DataFrame.from_dict(d, orient='index').T.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-09T13:29:28.302249Z",
     "start_time": "2018-06-09T13:29:28.294407Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (hack) Global configs\n",
    "conf_cache_disk = True\n",
    "conf_cache_memory = True\n",
    "conf_cache_fails = True\n",
    "\n",
    "class GetConf:\n",
    "    def __init__(self, splitAdj, divAdj, cache, mode, source, secondary):\n",
    "        self.splitAdj = splitAdj\n",
    "        self.divAdj = divAdj\n",
    "        self.cache = cache\n",
    "        self.mode = mode\n",
    "        self.source = source\n",
    "        self.secondary = secondary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-09T13:29:28.309304Z",
     "start_time": "2018-06-09T13:29:28.304393Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not \"fetchCache\" in globals():\n",
    "    fetchCache = {}\n",
    "    \n",
    "if not \"symbols_mem_cache\" in globals():\n",
    "    symbols_mem_cache = {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-09T13:29:28.327067Z",
     "start_time": "2018-06-09T13:29:28.311303Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Symbol:\n",
    "    def __init__(self, fullname):\n",
    "        self.fullname = fullname\n",
    "        parts = fullname.split(\"!\")\n",
    "        if len(parts) == 2:\n",
    "            fullname = parts[0]\n",
    "            self.currency = parts[1]\n",
    "        else:\n",
    "            self.currency = \"\"\n",
    "        parts = fullname.split(\"@\")\n",
    "        self.name = parts[0]\n",
    "        if len(parts) == 2:\n",
    "            self.source = parts[1]\n",
    "        else:\n",
    "            self.source = \"\"\n",
    "       \n",
    "    def __str__(self):\n",
    "        return self.fullname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-09T13:29:28.352088Z",
     "start_time": "2018-06-09T13:29:28.329804Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "from datetime import datetime as dt\n",
    "def xnpv(rate, values, dates):\n",
    "    '''Equivalent of Excel's XNPV function.\n",
    "\n",
    "    >>> from datetime import date\n",
    "    >>> dates = [date(2010, 12, 29), date(2012, 1, 25), date(2012, 3, 8)]\n",
    "    >>> values = [-10000, 20, 10100]\n",
    "    >>> xnpv(0.1, values, dates)\n",
    "    -966.4345...\n",
    "    '''\n",
    "    if rate <= -1.0:\n",
    "        return float('inf')\n",
    "    d0 = dates[0]    # or min(dates)\n",
    "    return sum([ vi / (1.0 + rate)**((di - d0).days / 365.0) for vi, di in zip(values, dates)])\n",
    "\n",
    "\n",
    "def xirr(values, dates):\n",
    "    '''Equivalent of Excel's XIRR function.\n",
    "\n",
    "    >>> from datetime import date\n",
    "    >>> dates = [date(2010, 12, 29), date(2012, 1, 25), date(2012, 3, 8)]\n",
    "    >>> values = [-10000, 20, 10100]\n",
    "    >>> xirr(values, dates)\n",
    "    0.0100612...\n",
    "    '''\n",
    "    # we prefer to try brentq first as newton keeps outputting tolerance warnings\n",
    "    try:\n",
    "        return scipy.optimize.brentq(lambda r: xnpv(r, values, dates), -1.0, 1e10)\n",
    "        #return scipy.optimize.newton(lambda r: xnpv(r, values, dates), 0.0, tol=0.0002)\n",
    "    except RuntimeError:    # Failed to converge?\n",
    "        return scipy.optimize.newton(lambda r: xnpv(r, values, dates), 0.0, tol=0.0002)\n",
    "        #return scipy.optimize.brentq(lambda r: xnpv(r, values, dates), -1.0, 1e10)\n",
    "\n",
    "#xirr([-100, 100, 200], [dt(2000, 1, 1), dt(2001, 1, 1), dt(2002, 1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-09T13:29:28.368075Z",
     "start_time": "2018-06-09T13:29:28.354260Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def curr_price(symbol):\n",
    "    if symbol in ignoredAssets: return 0\n",
    "    return get(symbol)[-1]\n",
    "\n",
    "#def getForex(fromCur, toCur):\n",
    "#    if fromCur == toCur: return 1\n",
    "#    if toCur == \"USD\":\n",
    "#        return get(fromCur + \"=X\", \"Y\")\n",
    "#    if fromCur == \"USD\":\n",
    "#        return get(toCur + \"=X\", \"Y\").map(lambda x: 1.0/x)\n",
    "\n",
    "def getForex(fromCur, toCur):\n",
    "    if fromCur == toCur: return 1\n",
    "    #tmp = get(fromCur + toCur + \"@CUR\").s\n",
    "    tmp = get(fromCur + \"/\" + toCur + \"@IC\").s\n",
    "    tmp = tmp.reindex(pd.date_range(start=tmp.index[0], end=tmp.index[-1]))\n",
    "    tmp = tmp.fillna(method=\"ffill\")\n",
    "    return tmp\n",
    "    #return wrap(tmp, fromCur+toCur)\n",
    "\n",
    "def convertSeries(s, fromCur, toCur):\n",
    "    if fromCur == toCur: return s\n",
    "    rate = getForex(fromCur, toCur)\n",
    "    s = (s*rate).dropna()\n",
    "    return s\n",
    "    \n",
    "def convertToday(value, fromCur, toCur):\n",
    "    if fromCur == toCur: return value\n",
    "    return value * getForex(fromCur, toCur)[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-09T18:26:45.587907Z",
     "start_time": "2018-06-09T18:26:40.945220Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getName(s):\n",
    "    if isinstance(s, str):\n",
    "        return s\n",
    "    return s.name\n",
    "\n",
    "def toSymbol(sym):\n",
    "    if isinstance(sym, Symbol):\n",
    "        return sym\n",
    "    if isinstance(sym, str):\n",
    "        return Symbol(sym)\n",
    "    assert False, \"invalid type for Symbol: \" + str(type(sym)) + \", \" + str(sym)\n",
    "\n",
    "class DataSource:\n",
    "    \n",
    "    def __init__(self, source):\n",
    "        self.source = source\n",
    "    \n",
    "    def fetch(self, symbol, conf):\n",
    "        pass\n",
    "    \n",
    "    def process(self, symbol, df, conf):\n",
    "        pass\n",
    "    \n",
    "    def get(self, symbol, conf):\n",
    "        global conf_cache_disk, conf_cache_memory, conf_cache_fails\n",
    "\n",
    "        df = None\n",
    "\n",
    "        # get from mem cache\n",
    "        if conf.cache and conf_cache_memory:\n",
    "            if symbol.fullname in symbols_mem_cache:\n",
    "                df = symbols_mem_cache[symbol.fullname]\n",
    "        \n",
    "        # get from disk cache\n",
    "        if df is None and conf.cache and conf_cache_disk:\n",
    "            df = cache_get(symbol, self.source)\n",
    "        \n",
    "        # attempt to fetch the symbol\n",
    "        if df is None:\n",
    "            failpath = cache_file(symbol, self.source) + \"._FAIL_\"\n",
    "            if os.path.isfile(failpath):\n",
    "                mtime = datetime.datetime.fromtimestamp(os.path.getmtime(failpath))\n",
    "                diff = datetime.datetime.now() - mtime\n",
    "                if conf_cache_fails and diff.total_seconds() <= 24 * 3600:\n",
    "                    raise Exception(\"Fetching has previously failed for {0}, will try again later\".format(symbol))\n",
    "\n",
    "            try:\n",
    "                # Attempt to actually fetch the symbol\n",
    "                if df is None:\n",
    "                    print(\"Fetching %s from %s .. \" % (symbol, self.source), end=\"\")\n",
    "                    df = self.fetch(symbol, conf)\n",
    "                    print(\"DONE\")\n",
    "                if df is None:\n",
    "                    print(\"FAILED\")\n",
    "                    raise Exception(\"Failed to fetch symbol: \" + str(symbol) + \" from \" + self.source)\n",
    "                if len(df) == 0:\n",
    "                    print(\"FAILED\")\n",
    "                    raise Exception(\"Symbol fetched but is empty: \" + str(symbol) + \" from \" + self.source)\n",
    "            except Exception as e:\n",
    "                # save a note that we failed\n",
    "                Path(failpath).touch()\n",
    "                raise Exception from e\n",
    "        \n",
    "        # write to disk cache\n",
    "        cache_set(symbol, self.source, df)\n",
    "        # write to mem cache\n",
    "        symbols_mem_cache[symbol.fullname] = df\n",
    "        \n",
    "        if conf.mode == \"raw\":\n",
    "            res = df\n",
    "        else:\n",
    "            res = self.process(symbol, df, conf)\n",
    "        return res.sort_index()\n",
    "\n",
    "fred_forex_codes = \"\"\"\n",
    "AUD\tDEXUSAL\n",
    "BRL\tDEXBZUS\n",
    "GBP\tDEXUSUK\n",
    "CAD\tDEXCAUS\n",
    "CNY\tDEXCHUS\n",
    "DKK\tDEXDNUS\n",
    "EUR\tDEXUSEU\n",
    "HKD\tDEXHKUS\n",
    "INR\tDEXINUS\n",
    "JPY\tDEXJPUS\n",
    "MYR\tDEXMAUS\n",
    "MXN\tDEXMXUS\n",
    "TWD\tDEXTAUS\n",
    "NOK\tDEXNOUS\n",
    "SGD\tDEXSIUS\n",
    "ZAR\tDEXSFUS\n",
    "KRW\tDEXKOUS\n",
    "LKR\tDEXSLUS\n",
    "SEK\tDEXSDUS\n",
    "CHF\tDEXSZUS\n",
    "VEF\tDEXVZUS\n",
    "\"\"\"\n",
    "\n",
    "boe_forex_codes = \"\"\"\n",
    "AUD\tXUDLADD\n",
    "CAD\tXUDLCDD\n",
    "CNY\tXUDLBK73\n",
    "CZK\tXUDLBK27\n",
    "DKK\tXUDLDKD\n",
    "HKD\tXUDLHDD\n",
    "HUF\tXUDLBK35\n",
    "INR\tXUDLBK64\n",
    "NIS\tXUDLBK65\n",
    "JPY\tXUDLJYD\n",
    "LTL\tXUDLBK38\n",
    "MYR\tXUDLBK66\n",
    "NZD\tXUDLNDD\n",
    "NOK\tXUDLNKD\n",
    "PLN\tXUDLBK49\n",
    "GBP\tXUDLGBD\n",
    "RUB\tXUDLBK69\n",
    "SAR\tXUDLSRD\n",
    "SGD\tXUDLSGD\n",
    "ZAR\tXUDLZRD\n",
    "KRW\tXUDLBK74\n",
    "SEK\tXUDLSKD\n",
    "CHF\tXUDLSFD\n",
    "TWD\tXUDLTWD\n",
    "THB\tXUDLBK72\n",
    "TRY\tXUDLBK75\n",
    "\"\"\"\n",
    "\n",
    "# https://blog.quandl.com/api-for-currency-data\n",
    "class ForexDataSource(DataSource):\n",
    "    def __init__(self, source):\n",
    "        self.fred_code_map = dict([s.split(\"\\t\") for s in fred_forex_codes.split(\"\\n\")[1:-1]])\n",
    "        self.boe_code_map = dict([s.split(\"\\t\") for s in boe_forex_codes.split(\"\\n\")[1:-1]])\n",
    "        self.boe_code_map[\"ILS\"] = self.boe_code_map[\"NIS\"]\n",
    "        super().__init__(source)\n",
    "    \n",
    "    def fetch(self, symbol, conf):\n",
    "        assert len(symbol.name) == 6\n",
    "        _from = symbol.name[:3]\n",
    "        _to = symbol.name[3:]\n",
    "        if _to != \"USD\" and _from != \"USD\":\n",
    "            raise Exception(\"Can only convert to/from USD\")\n",
    "        invert = _from == \"USD\"\n",
    "        curr = _to if invert else _from\n",
    "        \n",
    "        div100 = 1\n",
    "        if curr == \"GBC\":\n",
    "            div100 = 100\n",
    "            curr = \"GBP\"\n",
    "        \n",
    "        if curr in self.fred_code_map:\n",
    "            code = self.fred_code_map[curr]\n",
    "            df = quandl.get(\"FRED/\" + code)\n",
    "            if code.endswith(\"US\") != invert: # some of the FRED currencies are inverted vs the US dollar, argh..\n",
    "                df = df.apply(lambda x: 1.0/x)\n",
    "            return df / div100\n",
    "\n",
    "        if curr in self.boe_code_map:\n",
    "            code = self.boe_code_map[curr]\n",
    "            df = quandl.get(\"BOE/\" + code)\n",
    "            if not invert: # not sure if some of BEO currencies are NOT inverted vs USD, checked a few and they weren't\n",
    "                df = df.apply(lambda x: 1.0/x)\n",
    "            return df / div100\n",
    "\n",
    "        raise Exception(\"Currency pair is not supported: \" + symbol.name)\n",
    "        \n",
    "    def process(self, symbol, df, conf):\n",
    "        return df.iloc[:, 0]\n",
    "      \n",
    "# https://github.com/ranaroussi/fix-yahoo-finance\n",
    "class YahooDataSource(DataSource):\n",
    "    def fetch(self, symbol, conf):\n",
    "        return pdr.get_data_yahoo(symbol.name, progress=False, actions=True)\n",
    "\n",
    "    def process(self, symbol, df, conf):\n",
    "        if conf.mode == \"TR\":\n",
    "            assert conf.splitAdj and conf.divAdj\n",
    "            return df[\"Adj Close\"]\n",
    "        elif conf.mode == \"PR\":\n",
    "            # Yahoo \"Close\" data is split adjusted. \n",
    "            # We find the unadjusted data using the splits data\n",
    "            splitMul = df[\"Stock Splits\"][::-1].cumprod().shift().fillna(method=\"bfill\")\n",
    "            return df[\"Close\"] / splitMul        \n",
    "        elif conf.mode == \"divs\":\n",
    "            return df[\"Dividends\"]\n",
    "        else:\n",
    "            raise Exception(\"Unsupported mode [\" + conf.mode + \"] for YahooDataSource\")\n",
    "\n",
    "class QuandlDataSource(DataSource):\n",
    "    def fetch(self, symbol, conf):\n",
    "        return quandl.get(symbol.name)\n",
    "\n",
    "    def process(self, symbol, df, conf):\n",
    "        if \"Close\" in df.columns:\n",
    "            return df[\"Close\"]\n",
    "        return df.iloc[:, 0]\n",
    "\n",
    "    \n",
    "class GoogleDataSource(DataSource):\n",
    "    def fetch(self, symbol, conf):\n",
    "        return pandas_datareader.data.DataReader(symbol.name, 'google')\n",
    "\n",
    "    def process(self, symbol, df, conf):\n",
    "        return df[\"Close\"]\n",
    "    \n",
    "AV_API_KEY = 'BB18'\n",
    "class AlphaVantageDataSource(DataSource):\n",
    "\n",
    "    def adjustSplits(self, price, splits):\n",
    "        r = splits.cumprod()\n",
    "        return price * r\n",
    "    \n",
    "    # AV sometimes have duplicate split multiplers, we only use the last one \n",
    "    def fixAVSplits(self, df):\n",
    "        df = df.sort_index()\n",
    "        split = df[\"8. split coefficient\"]\n",
    "        count = 0\n",
    "        for t, s in list(split.items())[::-1]:\n",
    "            if s == 1.0:\n",
    "                count = 0\n",
    "                continue\n",
    "            count += 1\n",
    "            if count == 1:\n",
    "                continue\n",
    "            if count > 1:\n",
    "                split[t] = 1.0\n",
    "        df[\"8. split coefficient\"] = split\n",
    "        return df\n",
    "\n",
    "    def fetch(self, symbol, conf):\n",
    "        ts = TimeSeries(key=AV_API_KEY, output_format='pandas')\n",
    "        df, meta_data = ts.get_daily_adjusted(symbol.name, outputsize=\"full\")\n",
    "        df.index = pd.to_datetime(df.index, format=\"%Y-%m-%d\")\n",
    "        df = self.fixAVSplits(df)\n",
    "        return df\n",
    "\n",
    "    def process(self, symbol, df, conf):\n",
    "        if conf.mode == \"TR\":\n",
    "            return df[\"5. adjusted close\"]\n",
    "        elif conf.mode == \"PR\":\n",
    "            return self.adjustSplits(df[\"4. close\"], df['8. split coefficient'])\n",
    "        elif conf.mode == \"divs\":\n",
    "            return df[\"7. dividend amount\"]\n",
    "        else:\n",
    "            raise Exception(\"Unsupported mode [\" + conf.mode + \"] for AlphaVantageDataSource\")\n",
    "        \n",
    "class AlphaVantageCryptoDataSource(DataSource):\n",
    "\n",
    "    def fetch(self, symbol, conf):\n",
    "        cc = CryptoCurrencies(key=AV_API_KEY, output_format='pandas')\n",
    "        df, meta_data = cc.get_digital_currency_daily(symbol=symbol.name, market='USD')\n",
    "        df.index = pd.to_datetime(df.index, format=\"%Y-%m-%d\")\n",
    "        return df\n",
    "\n",
    "    def process(self, symbol, df, conf):\n",
    "        return df['4a. close (USD)']\n",
    "\n",
    "class CryptoCompareDataSource(DataSource):\n",
    "    def fetch(self, symbol, conf):\n",
    "        url = \"https://min-api.cryptocompare.com/data/histoday?fsym=__sym__&tsym=USD&limit=600000&aggregate=1&e=CCCAGG\"\n",
    "        d = json.loads(requests.get(url.replace(\"__sym__\", symbol.name)).text)\n",
    "        df = pd.DataFrame(d[\"Data\"])\n",
    "        if len(df) == 0:\n",
    "            return None\n",
    "        df[\"time\"] = pd.to_datetime(df.time, unit=\"s\")\n",
    "        df.set_index(\"time\", inplace=True)\n",
    "        return df\n",
    "\n",
    "    def process(self, symbol, df, conf):\n",
    "        return df.close\n",
    "\n",
    "# NOTE: data is SPLIT adjusted, but has no dividends and is NOT DIVIDEND adjusted \n",
    "# NOTE: it has data all the way to the start, but returned result is capped in length for ~20 years\n",
    "#       and results are trimmed from the END, not from the start. TBD to handle this properly.\n",
    "#       for now we start at 1.1.2000\n",
    "class InvestingComDataSource(DataSource):\n",
    "\n",
    "    def getUrl(self, symbol):\n",
    "        symbol = symbol.name\n",
    "        data = {\n",
    "            'search_text': symbol,\n",
    "            'term': symbol, \n",
    "            'country_id': '0',\n",
    "            'tab_id': 'All'\n",
    "        }\n",
    "        headers = {\n",
    "                    'Origin': 'https://www.investing.com',\n",
    "                    'Accept-Encoding': 'gzip, deflate, br',\n",
    "                    'Accept-Language': 'en-US,en;q=0.9,he;q=0.8',\n",
    "                    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.117 Safari/537.36',\n",
    "                    'Content-Type': 'application/x-www-form-urlencoded',\n",
    "                    'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "                    'Referer': 'https://www.investing.com/search?q=' + symbol,\n",
    "                    'X-Requested-With': 'XMLHttpRequest',\n",
    "                    'Connection': 'keep-alive'    \n",
    "                }\n",
    "        r = requests.post(\"https://www.investing.com/search/service/search\", data=data, headers=headers)\n",
    "        res = r.text\n",
    "        res = json.loads(res)\n",
    "        return res[\"All\"][0][\"link\"]\n",
    "    \n",
    "    def getCodes(self, url):\n",
    "        url = \"https://www.investing.com\" + url + \"-historical-data\"\n",
    "        \n",
    "        headers = {\n",
    "                    'Origin': 'https://www.investing.com',\n",
    "                    'Accept-Encoding': 'gzip, deflate, br',\n",
    "                    'Accept-Language': 'en-US,en;q=0.9,he;q=0.8',\n",
    "                    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.117 Safari/537.36',\n",
    "                    'Content-Type': 'application/x-www-form-urlencoded',\n",
    "                    'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "                    'Referer': 'https://www.investing.com/',\n",
    "                    'X-Requested-With': 'XMLHttpRequest',\n",
    "                    'Connection': 'keep-alive'    \n",
    "                }\n",
    "        r = requests.get(url,headers=headers)\n",
    "        text = r.text\n",
    "        \n",
    "        m = re.search(\"smlId:\\s+(\\d+)\", text)\n",
    "        smlId = m.group(1)\n",
    "        \n",
    "        m = re.search(\"pairId:\\s+(\\d+)\", text)\n",
    "        pairId = m.group(1)\n",
    "        \n",
    "        return pairId, smlId\n",
    "    \n",
    "    def getHtml(self, pairId, smlId):\n",
    "        data = [\n",
    "            'curr_id=' + pairId,\n",
    "            'smlID=' + smlId,\n",
    "            'header=',\n",
    "            'st_date=01%2F01%2F2000',\n",
    "            'end_date=01%2F01%2F2100',\n",
    "            'interval_sec=Daily',\n",
    "            'sort_col=date',\n",
    "            'sort_ord=DESC', \n",
    "            'action=historical_data'\n",
    "        ]\n",
    "        data = \"&\".join(data)\n",
    "        headers = {\n",
    "            'Origin': 'https://www.investing.com',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept-Language': 'en-US,en;q=0.9,he;q=0.8',\n",
    "            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.117 Safari/537.36',\n",
    "            'Content-Type': 'application/x-www-form-urlencoded',\n",
    "            'Accept': 'text/plain, */*; q=0.01',\n",
    "            'Referer': 'https://www.investing.com/',\n",
    "            'X-Requested-With': 'XMLHttpRequest',\n",
    "            'Connection': 'keep-alive'    \n",
    "        }\n",
    "        r = requests.post(\"https://www.investing.com/instruments/HistoricalDataAjax\", data=data, headers=headers)\n",
    "        return r.text\n",
    "    \n",
    "    def fetch(self, symbol, conf):\n",
    "        symbolUrl = self.getUrl(symbol)\n",
    "        \n",
    "        pairId, smlId = self.getCodes(symbolUrl)\n",
    "        \n",
    "        html = self.getHtml(pairId, smlId)\n",
    "        #print(html)\n",
    "        parsed_html = BeautifulSoup(html, \"lxml\")\n",
    "        df = pd.DataFrame(columns=[\"date\", \"price\"])\n",
    "        \n",
    "        for i, tr in enumerate(parsed_html.find_all(\"tr\")[1:]): # skip header\n",
    "            data = [x.get(\"data-real-value\") for x in tr.find_all(\"td\")]\n",
    "            if len(data) == 0 or data[0] is None:\n",
    "                continue\n",
    "            date = datetime.datetime.utcfromtimestamp(int(data[0]))\n",
    "            close = float(data[1].replace(\",\", \"\"))\n",
    "            #open = data[2]\n",
    "            #high = data[3]\n",
    "            #low = data[4]\n",
    "            #volume = data[5]\n",
    "            df.loc[i] = [date, close]\n",
    "            \n",
    "        df = df.set_index(\"date\")\n",
    "        return df\n",
    "\n",
    "    def process(self, symbol, df, conf):\n",
    "        return df['price']\n",
    "\n",
    "import time    \n",
    "class JustEtfDataSource(DataSource):\n",
    "\n",
    "\n",
    "    def parseDate(self, s):\n",
    "        s = s.strip(\" {x:Date.UTC()\")\n",
    "        p = [int(x) for x in s.split(\",\")]\n",
    "        dt = datetime.datetime(p[0], p[1] + 1, p[2])\n",
    "        return dt\n",
    "\n",
    "    def parseDividends(self, x):\n",
    "        x = re.split(\"data: \\[\", x)[1]\n",
    "        x = re.split(\"\\]\\^,\", x)[0]\n",
    "        data = []\n",
    "        for x in re.split(\"\\},\\{\", x):\n",
    "            p = re.split(\", events: \\{click: function\\(\\) \\{  \\}\\}, title: 'D', text: 'Dividend \", x)\n",
    "            dt = self.parseDate(p[0])\n",
    "            p = p[1].strip(\"',id: }\").split(\" \")\n",
    "            currency = p[0]\n",
    "            value = float(p[1])\n",
    "            data.append((dt, value))\n",
    "        return pd.DataFrame(data, columns=['dt', 'divs']).set_index(\"dt\")\n",
    "\n",
    "    def parsePrice(self, s):\n",
    "        data = []\n",
    "        line = s\n",
    "        t = \"data: [\"\n",
    "        line = line[line.find(t) + len(t):]\n",
    "        t = \"^]^\"\n",
    "        line = line[:line.find(t)]\n",
    "        #print(line)\n",
    "        parts = line.split(\"^\")\n",
    "        for p in parts:\n",
    "            p = p.strip(\"[],\")\n",
    "            p = p.split(\")\")\n",
    "            value = float(p[1].replace(\",\", \"\"))\n",
    "            dateStr = p[0].split(\"(\")[1]\n",
    "            p = [int(x) for x in dateStr.split(\",\")]\n",
    "            dt = datetime.datetime(p[0], p[1] + 1, p[2])\n",
    "            data.append((dt, value))\n",
    "            #print(dt, value)\n",
    "        df = pd.DataFrame(data, columns=['dt', 'price']).set_index(\"dt\")\n",
    "        return df\n",
    "\n",
    "    def parseRawText(self, s):\n",
    "        x = re.split(\"addSeries\", s)\n",
    "        df = self.parsePrice(x[1])\n",
    "        divs = self.parseDividends(x[2])\n",
    "        df[\"divs\"] = divs[\"divs\"]\n",
    "        return df\n",
    "\n",
    "    def getIsin(self, symbol):\n",
    "        symbolName = symbol.name\n",
    "        data = {\n",
    "            'draw': '1',\n",
    "            'start': '0', \n",
    "            'length': '25', \n",
    "            'search[regex]': 'false', \n",
    "            'lang': 'en', \n",
    "            'country': 'GB', \n",
    "            'universeType': 'private', \n",
    "            'etfsParams': 'query=' + symbolName, \n",
    "            'groupField': 'index', \n",
    "        }\n",
    "        headers = {\n",
    "                    'Accept-Encoding': 'gzip, deflate, br',\n",
    "                }\n",
    "        session = requests.Session()\n",
    "        \n",
    "        \n",
    "        r = session.get(\"https://www.justetf.com/en/etf-profile.html?tab=chart&isin=IE00B5L65R35\", headers=headers)\n",
    "        \n",
    "        r = session.post(\"https://www.justetf.com/servlet/etfs-table\", data=data, headers=headers)\n",
    "        res = r.text\n",
    "        \n",
    "        res = json.loads(res)\n",
    "        for d in res[\"data\"]:\n",
    "            if d[\"ticker\"] == symbolName:\n",
    "                return (d[\"isin\"], session)\n",
    "        raise Exception(\"Symbol not found in source: \" + str(symbol))\n",
    "    \n",
    "    def getData(self, isin, session, conf, raw=False):\n",
    "        if not session:\n",
    "            session = requests.Session()\n",
    "            \n",
    "        headers = {\n",
    "                    'Accept-Encoding': 'gzip, deflate, br',\n",
    "                }\n",
    "        \n",
    "        url3 = \"https://www.justetf.com/uk/etf-profile.html?groupField=index&from=search&isin=\" + isin + \"&tab=chart\"\n",
    "        r = session.get(url3, headers=headers)\n",
    "\n",
    "        r = session.get(\"https://www.justetf.com/sw.js\", headers=headers)\n",
    "        text = r.text\n",
    "\n",
    "        headers = {\n",
    "            'accept-encoding': 'gzip, deflate, br',\n",
    "            'accept-language': 'en-US,en;q=0.9,he;q=0.8',\n",
    "            'wicket-focusedelementid': 'id1b',\n",
    "            'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36',\n",
    "            'accept': 'text/xml',\n",
    "            'referer': 'https://www.justetf.com/uk/etf-profile.html?groupField=index&from=search&isin=IE00B5L65R35&tab=chart',\n",
    "            'authority': 'www.justetf.com',\n",
    "            'wicket-ajax': 'true'\n",
    "        }\n",
    "\n",
    "        if conf.mode == \"PR\":\n",
    "            headers[\"wicket-focusedelementid\"] = \"includePayment\"\n",
    "            url = \"https://www.justetf.com/uk/?wicket:interface=:2:tabs:panel:chart:optionsPanel:selectContainer:includePaymentContainer:includePayment::IBehaviorListener:0:1&random=0.8852086768595453\"\n",
    "            r = session.post(url, headers=headers)\n",
    "            text = r.text\n",
    "            headers[\"wicket-focusedelementid\"] = \"id1b\"\n",
    "        \n",
    "        url = \"https://www.justetf.com/en/?wicket:interface=:0:tabs:panel:chart:dates:ptl_3y::IBehaviorListener:0:1&random=0.2525050377785838\"\n",
    "        r = session.get(url, headers=headers)\n",
    "        text = r.text\n",
    "\n",
    "        \n",
    "        # PRICE (instead of percent change)\n",
    "        data = { 'tabs:panel:chart:optionsPanel:selectContainer:valueType': 'market_value' }\n",
    "        url = \"https://www.justetf.com/en/?wicket:interface=:0:tabs:panel:chart:optionsPanel:selectContainer:valueType::IBehaviorListener:0:1&random=0.7560635418741075\"\n",
    "        r = session.post(url, headers=headers, data=data)\n",
    "        text = r.text\n",
    "        \n",
    "        # CURRENCY\n",
    "        #data = { 'tabs:panel:chart:optionsPanel:selectContainer:currencies': '3' }\n",
    "        #url = \"https://www.justetf.com/en/?wicket:interface=:0:tabs:panel:chart:optionsPanel:selectContainer:currencies::IBehaviorListener:0:1&random=0.8898086171718949\"\n",
    "        #r = session.post(url, headers=headers, data=data)\n",
    "        #text = r.text\n",
    "        \n",
    "        \n",
    "        \n",
    "        url = \"https://www.justetf.com/en/?wicket:interface=:0:tabs:panel:chart:dates:ptl_max::IBehaviorListener:0:1&random=0.2525050377785838\"\n",
    "        #url = \"https://www.justetf.com/uk/?wicket:interface=:3:tabs:panel:chart:dates:ptl_max::IBehaviorListener:0:1\"\n",
    "        \n",
    "        #plain_cookie = 'locale_=en_GB; universeCountry_=GB; universeDisclaimerAccepted_=false; JSESSIONID=5C4770C8CE62E823C17E292486D04112.production01; AWSALB=Wy2YQ+nfXWR+lTtsGly/hBDFD5pCCtYo/VxE0lIXBPlA/SdQDbRxhg+0q2E8UybYawqQiy3/1m2Bs4xvN8yFW3cs/2zy8385MuhGGCN/FUwnstSvbL7T8rfcV03k'\n",
    "        #cj = requests.utils.cookiejar_from_dict(dict(p.split('=') for p in plain_cookie.split('; ')))\n",
    "        #session.cookies = cj\n",
    "        \n",
    "        r = session.get(url, headers=headers)\n",
    "        text = r.text\n",
    "        #print(text)\n",
    "        if raw:\n",
    "            return text\n",
    "        \n",
    "        return self.parseRawText(text)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def fetch(self, symbol, conf):\n",
    "        return self.getData(symbol.name, None, conf)\n",
    "\n",
    "    def process(self, symbol, df, conf):\n",
    "        if conf.mode == \"TR\":\n",
    "            return df[\"price\"]\n",
    "        elif conf.mode == \"PR\":\n",
    "            raise Exception(\"Unsupported mode [\" + conf.mode + \"] for JustEtfDataSource\")\n",
    "        elif conf.mode == \"divs\":\n",
    "            return df[\"divs\"]\n",
    "        else:\n",
    "            raise Exception(\"Unsupported mode [\" + conf.mode + \"] for JustEtfDataSource\")\n",
    "        \n",
    "        return df['price']\n",
    "    \n",
    "#x = JustEtfDataSource(\"XXX\")\n",
    "#isin, session = x.getIsin(Symbol(\"ERNS\"))\n",
    "#t = x.getData(isin, session)\n",
    "\n",
    "#conf = lambda x: x\n",
    "#conf.mode = \"TR\"\n",
    "#t = x.getData(\"IE00B5L65R35\", None, conf, True)\n",
    "\n",
    "class BloombergDataSource(DataSource):\n",
    "    def fetch(self, symbol, conf):\n",
    "        url = \"https://www.bloomberg.com/markets/api/bulk-time-series/price/__sym__?timeFrame=5_YEAR\"\n",
    "        sym = symbol.name.replace(\";\", \":\")\n",
    "        d = json.loads(requests.get(url.replace(\"__sym__\", sym)).text)\n",
    "        #print(d)\n",
    "        df = pd.DataFrame(d[0][\"price\"])\n",
    "        if len(df) == 0:\n",
    "            return None\n",
    "        df[\"date\"] = pd.to_datetime(df.date, format=\"%Y-%m-%d\")\n",
    "        df.set_index(\"date\", inplace=True)\n",
    "        return df\n",
    "\n",
    "    def process(self, symbol, df, conf):\n",
    "        return df.value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T20:31:50.649755Z",
     "start_time": "2018-03-15T20:31:50.293516Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fetching data\n",
    "\n",
    "if not \"Wrapper\" in locals():\n",
    "    class Wrapper(object):\n",
    "\n",
    "        def __init__(self, s):\n",
    "            #self.s = s\n",
    "            object.__setattr__(self, \"s\", s)\n",
    "\n",
    "        def __getattr__(self, name):\n",
    "            attr = self.s.__getattribute__(name)\n",
    "\n",
    "            if hasattr(attr, '__call__'):\n",
    "                def newfunc(*args, **kwargs):\n",
    "                    result = attr(*args, **kwargs)\n",
    "                    if type(result) is pd.Series:\n",
    "                        result = Wrapper(result)\n",
    "                    return result\n",
    "                return newfunc\n",
    "\n",
    "            if type(attr) is pd.Series:\n",
    "                attr = Wrapper(attr)\n",
    "            return attr\n",
    "\n",
    "        def __setattr__(self, name, value):\n",
    "            self.s.__setattr__(name, value)\n",
    "\n",
    "        def __getitem__(self, item):\n",
    "             return wrap(self.s.__getitem__(item), self.s.name)\n",
    "\n",
    "#         def __truediv__(self, other):\n",
    "#             divisor = other\n",
    "#             if type(other) is Wrapper:\n",
    "#                 divisor = other.s\n",
    "#             series = self.s / divisor\n",
    "#             name = self.name\n",
    "#             if type(other) is Wrapper:\n",
    "#                 name = self.s.name + \" / \" + other.s.name\n",
    "#             return wrap(series, name)\n",
    "\n",
    "        def __truediv__(self, other):\n",
    "            return Wrapper.doop(self, other, \"/\", lambda x, y: x / y)\n",
    "        def __rtruediv__(self, other):\n",
    "            return Wrapper.doop(self, other, \"/\", lambda x, y: x / y, right=True)\n",
    "        \n",
    "        def doop(self, other, opname, opLambda, right=False):\n",
    "            divisor = other\n",
    "            if type(other) is Wrapper:\n",
    "                divisor = other.s\n",
    "            if right:\n",
    "                series = opLambda(divisor, self.s)\n",
    "            else:\n",
    "                series = opLambda(self.s, divisor)\n",
    "            name = self.name\n",
    "            if type(other) is Wrapper:\n",
    "                if right:\n",
    "                    name = other.s.name + \" \" + opname + \" \" + self.s.name\n",
    "                else:\n",
    "                    name = self.s.name + \" \" + opname + \" \" + other.s.name\n",
    "            return wrap(series, name)\n",
    "\n",
    "        def __sub__(self, other):\n",
    "            return Wrapper.doop(self, other, \"-\", lambda x, y: x - y)\n",
    "        #def __rsub__(self, other):\n",
    "        #    return Wrapper.doop(self, other, \"-\", lambda x, y: x - y, right=True)\n",
    "\n",
    "        def __mul__(self, other):\n",
    "            return Wrapper.doop(self, other, \"*\", lambda x, y: x * y)\n",
    "        def __rmul__(self, other):\n",
    "            return Wrapper.doop(self, other, \"*\", lambda x, y: x * y, right=True)\n",
    "\n",
    "def wrap(s, name=\"\"):\n",
    "    name = name or s.name\n",
    "    if not name:\n",
    "        raise Exception(\"no name\")\n",
    "    if isinstance(s, pd.Series):\n",
    "        s = Wrapper(s)\n",
    "        s.name = name\n",
    "    elif isinstance(s, Wrapper):\n",
    "        s.name = name\n",
    "    return s\n",
    "\n",
    "name = wrap # syn-sugar\n",
    "\n",
    "    \n",
    "data_sources = {\n",
    "    \n",
    "    \"B\": BloombergDataSource(\"B\"),\n",
    "    \"JT\": JustEtfDataSource(\"JT\"),\n",
    "    \"Y\": YahooDataSource(\"Y\"),\n",
    "    \"IC\": InvestingComDataSource(\"IC\"),\n",
    "    \"Q\": QuandlDataSource(\"Q\"),\n",
    "    \"AV\": AlphaVantageDataSource(\"AV\"),\n",
    "    \"CC\": CryptoCompareDataSource(\"CC\"),\n",
    "    \"CCAV\": AlphaVantageCryptoDataSource(\"CCAV\"),\n",
    "    \"CUR\": ForexDataSource(\"CUR\"),\n",
    "    \"G\": GoogleDataSource(\"G\")\n",
    "               }\n",
    "\n",
    "def getFrom(symbol, conf):\n",
    "    # special handling for forex\n",
    "    # if a match, if will recurse and return here with XXXUSD@CUR\n",
    "    if len(symbol.name) == 6 and not symbol.source:\n",
    "        parts = symbol.name[:3], symbol.name[3:]\n",
    "        if parts[0] == \"USD\" or parts[1] == \"USD\":\n",
    "            return wrap(getForex(parts[0], parts[1]), symbol.name)\n",
    "    \n",
    "    source = symbol.source or conf.source or \"AV\"\n",
    "    if not source in data_sources:\n",
    "        raise Exception(\"Unsupported source: \" + source)\n",
    "    if not conf.secondary:\n",
    "        return data_sources[source].get(symbol, conf)\n",
    "    try:\n",
    "        return data_sources[source].get(symbol, conf)\n",
    "    except Exception as e:\n",
    "        # if the source wasn't explicitly stated, try from secondary\n",
    "        if not symbol.source and not conf.source:\n",
    "            print(\"Failed to fetch {0} from {1}, trying from {2} .. \".format(symbol, source, conf.secondary), end=\"\")\n",
    "            res = data_sources[conf.secondary].get(symbol, conf)\n",
    "            print(\"DONE\")\n",
    "            return res\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "def format_filename(s):\n",
    "    valid_chars = \"-_.() %s%s\" % (string.ascii_letters, string.digits)\n",
    "    filename = ''.join(c for c in s if c in valid_chars)\n",
    "    filename = filename.replace(' ','_')\n",
    "    return filename\n",
    "    \n",
    "def cache_file(symbol, source):\n",
    "    filepath = os.path.join(\"symbols\", source, format_filename(symbol.name))\n",
    "    dirpath = os.path.dirname(filepath)\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "    return filepath\n",
    "\n",
    "def cache_get(symbol, source):\n",
    "    filepath = cache_file(symbol, source)\n",
    "    if os.path.exists(filepath):\n",
    "        #res = pd.read_csv(filepath, squeeze=True, names=[\"date\", \"value\"], index_col=\"date\")\n",
    "        res = pd.read_csv(filepath, squeeze=False, index_col=\"date\")\n",
    "        res.index = pd.to_datetime(res.index, format=\"%Y-%m-%d\")\n",
    "        return res\n",
    "    return None\n",
    "\n",
    "def cache_set(symbol, source, s):\n",
    "    filepath = cache_file(symbol, source)\n",
    "    s.to_csv(filepath, date_format=\"%Y-%m-%d\", index_label=\"date\")\n",
    "\n",
    "\n",
    "def get_port(d, name, getArgs):\n",
    "    if isinstance(d, str):\n",
    "        res = parse_portfolio_def(d)\n",
    "        if not res:\n",
    "            raise Exception(\"Invalid portfolio definition: \" + d)\n",
    "        d = res\n",
    "    if not isinstance(d, dict):\n",
    "        raise Exception(\"Portfolio definition must be str or dict, was: \" + type(d))        \n",
    "    if isinstance(name, dict):\n",
    "        name = \"|\".join([getName(k)+\":\"+str(v) for k, v in d.items()])\n",
    "    df = pd.DataFrame(logret(get(k, **getArgs).s)*v/100 for k,v in d.items()).T.dropna()\n",
    "    res = Wrapper(i_logret(df.sum(axis=1)))\n",
    "    res.name = name\n",
    "    return res\n",
    "\n",
    "def parse_portfolio_def(s):\n",
    "    if isinstance(s, dict):\n",
    "        return s\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    d = {}\n",
    "    parts = s.split(\"|\")\n",
    "    for p in parts:\n",
    "        parts2 = p.split(\":\")\n",
    "        if len(parts2) != 2:\n",
    "            return None\n",
    "        d[parts2[0]] = float(parts2[1])\n",
    "    return d\n",
    "\n",
    "def getNtr(s, getArgs):\n",
    "    mode = getArgs[\"mode\"]\n",
    "    getArgs[\"mode\"] = \"PR\"\n",
    "    pr = get(s, **getArgs)\n",
    "    getArgs[\"mode\"] = \"divs\"\n",
    "    divs = get(s, **getArgs)\n",
    "    getArgs[\"mode\"] = mode\n",
    "    \n",
    "    tax = 0.25\n",
    "    divs = divs * (1-tax)\n",
    "    divs = divs / pr\n",
    "    divs = divs.fillna(0)\n",
    "    r = 1 + divs.s\n",
    "    r = r.cumprod()\n",
    "    ntr = (pr * r).dropna()\n",
    "    #ntr = wrap(ntr, s.name + \" NTR\")\n",
    "    ntr = wrap(ntr, s.name)\n",
    "    return ntr\n",
    "\n",
    "\n",
    "def get(symbol, source=None, cache=True, splitAdj=True, divAdj=True, adj=None, mode=\"TR\", secondary=\"Y\", fillDays=True, despike=False, trim=False):\n",
    "    getArgs = {}\n",
    "    getArgs[\"source\"] = source\n",
    "    getArgs[\"cache\"] = cache\n",
    "    getArgs[\"splitAdj\"] = splitAdj\n",
    "    getArgs[\"divAdj\"] = divAdj\n",
    "    getArgs[\"adj\"] = adj\n",
    "    getArgs[\"mode\"] = mode\n",
    "    getArgs[\"secondary\"] = secondary\n",
    "    getArgs[\"fillDays\"] = fillDays\n",
    "    getArgs[\"despike\"] = despike\n",
    "    getArgs[\"trim\"] = trim\n",
    "    \n",
    "    if isinstance(symbol, list):\n",
    "        lst = symbol\n",
    "        lst = [get(s, **getArgs) for s in lst]\n",
    "        if despike:\n",
    "            lst = [globals()[\"despike\"](s) for s in lst]\n",
    "        if trim:\n",
    "            lst = doTrim(lst)\n",
    "        return lst\n",
    "    \n",
    "    # support for yield period tuples, e.g.: (SPY, 4)\n",
    "    if isinstance(symbol, tuple) and len(symbol) == 2:\n",
    "        symbol, _ = symbol\n",
    "    \n",
    "    if isinstance(symbol, Wrapper) or isinstance(symbol, pd.Series):\n",
    "        return symbol\n",
    "    if \"ignoredAssets\" in globals() and ignoredAssets and symbol in ignoredAssets:\n",
    "        return wrap(pd.Series(), \"<empty>\")\n",
    "    \n",
    "    # special handing for composite portfolios\n",
    "    port = parse_portfolio_def(symbol)\n",
    "    if port:\n",
    "        return get_port(port, symbol, getArgs)\n",
    "    \n",
    "    symbol = toSymbol(symbol)\n",
    "    \n",
    "    if mode == \"NTR\":\n",
    "        return getNtr(symbol, getArgs)\n",
    "    \n",
    "    if adj == False:\n",
    "        splitAdj = False\n",
    "        divAdj = False\n",
    "\n",
    "    s = getFrom(symbol, GetConf(splitAdj, divAdj, cache, mode, source, secondary))\n",
    "    \n",
    "    s = s[s>0] # clean up broken yahoo data, etc ..\n",
    "    \n",
    "    if fillDays and mode != \"divs\" and mode != \"raw\":\n",
    "        s = s.reindex(pd.date_range(start=s.index[0], end=s.index[-1]))\n",
    "        s = s.interpolate()\n",
    "    \n",
    "    return wrap(s, symbol.fullname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T20:31:50.669012Z",
     "start_time": "2018-03-15T20:31:50.652011Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     def __getattribute__(self,name):\n",
    "#         s = object.__getattribute__(self, \"s\")\n",
    "#         if name == \"s\":\n",
    "#             return s\n",
    "        \n",
    "#         attr = s.__getattribute__(name)\n",
    "        \n",
    "#         if hasattr(attr, '__call__'):\n",
    "#             def newfunc(*args, **kwargs):\n",
    "#                 result = attr(*args, **kwargs)\n",
    "#                 if type(result) is pd.Series:\n",
    "#                     result = Wrapper(result)\n",
    "#                 return result\n",
    "#             return newfunc\n",
    "        \n",
    "#         if type(attr) is pd.Series:\n",
    "#             attr = Wrapper(attr)\n",
    "#         return attr\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T20:31:50.745663Z",
     "start_time": "2018-03-15T20:31:50.672748Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "def createVerticalLine(xval):\n",
    "    shape = {\n",
    "            'type': 'line',\n",
    "            #'xref': 'x',\n",
    "            'x0': xval,\n",
    "            'x1': xval,\n",
    "            'yref': 'paper',\n",
    "            'y0': 0,\n",
    "            'y1': 1,\n",
    "            #'fillcolor': 'blue',\n",
    "            'opacity': 1,\n",
    "            'line': {\n",
    "                'width': 1,\n",
    "                'color': 'red'\n",
    "            }\n",
    "        }\n",
    "    return shape\n",
    "    \n",
    "def createHorizontalLine(yval):\n",
    "    shape = {\n",
    "            'type': 'line',\n",
    "            'xref': 'paper',\n",
    "            'x0': 0,\n",
    "            'x1': 1,\n",
    "            #'yref': 'x',\n",
    "            'y0': yval,\n",
    "            'y1': yval,\n",
    "            #'fillcolor': 'blue',\n",
    "            'opacity': 1,\n",
    "            'line': {\n",
    "                'width': 1,\n",
    "                'color': 'red'\n",
    "            }\n",
    "        }\n",
    "    return shape\n",
    "    \n",
    "def plot(*arr, log=True, title=None, legend=True):\n",
    "    data = []\n",
    "    shapes = []\n",
    "    for val in arr:\n",
    "        # series\n",
    "        if isinstance(val, Wrapper):\n",
    "            data.append(go.Scatter(x=val.index, y=val.s, name=val.name, text=val.name))\n",
    "        elif isinstance(val, pd.Series):\n",
    "            data.append(go.Scatter(x=val.index, y=val, name=val.name, text=val.name))\n",
    "        # vertical date line\n",
    "        elif isinstance(val, datetime.datetime):\n",
    "            shapes.append(createVerticalLine(val))\n",
    "        # vertical date line\n",
    "        elif isinstance(val, np.datetime64):\n",
    "            shapes.append(createVerticalLine(val.astype(datetime.datetime)))\n",
    "        # horizontal value line\n",
    "        elif isinstance(val, numbers.Real):\n",
    "            shapes.append(createHorizontalLine(val))\n",
    "        else:\n",
    "            raise Exception(\"unsupported value type: \" + str(type(val)))\n",
    "    \n",
    "    for d in data:\n",
    "        d = d.y\n",
    "        if isinstance(d, Wrapper):\n",
    "            d = d.s\n",
    "        if np.any(d <= 0):\n",
    "            log = False\n",
    "            \n",
    "    mar = 30\n",
    "    margin=gol.Margin(\n",
    "        l=mar,\n",
    "        r=mar,\n",
    "        b=mar,\n",
    "        t=mar,\n",
    "        pad=0\n",
    "    )\n",
    "    \n",
    "    #bgcolor='#FFFFFFBB',bordercolor='#888888',borderwidth=1,\n",
    "    if legend:\n",
    "        legendArgs=dict(x=0,y=1,traceorder='normal',\n",
    "            bgcolor='rgb(255,255,255,187)',bordercolor='#888888',borderwidth=1,\n",
    "            font=dict(family='sans-serif',size=12,color='#000'),\n",
    "        )    \n",
    "    else:\n",
    "        legendArgs = {}\n",
    "    yaxisScale = \"log\" if log else None\n",
    "    layout = go.Layout(legend=legendArgs, \n",
    "                       showlegend=legend, \n",
    "                       margin=margin, \n",
    "                       yaxis=dict(type=yaxisScale, autorange=True), \n",
    "                       shapes=shapes, \n",
    "                       title=title,\n",
    "                       hovermode = 'closest')\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(fig)\n",
    "\n",
    "# show a stacked area chart normalized to 100% of multiple time series\n",
    "def plotly_area(df, title=None):\n",
    "    tt = df.div(df.sum(axis=1), axis=0)*100 # normalize to summ 100\n",
    "    tt = tt.reindex(tt.mean().sort_values(ascending=False).index, axis=1) # sort columns by mean value\n",
    "    tt = tt.sort_index()\n",
    "    tt2 = tt.cumsum(axis=1) # calc cum-sum\n",
    "    data = []\n",
    "    for col in tt2:\n",
    "        s = tt2[col]\n",
    "        trace = go.Scatter(\n",
    "            name=col,\n",
    "            x=s.index.to_datetime(),\n",
    "            y=s.values,\n",
    "            text=[\"{:.1f}%\".format(v) for v in tt[col].values], # use text as non-cumsum values\n",
    "            hoverinfo='name+x+text',\n",
    "            mode='lines',\n",
    "            fill='tonexty'\n",
    "        )\n",
    "        data.append(trace)\n",
    "\n",
    "    mar = 30\n",
    "    margin=gol.Margin(l=mar,r=mar,b=mar,t=mar,pad=0)\n",
    "    legend=dict(x=0,y=1,traceorder='reversed',\n",
    "        bgcolor='#FFFFFFBB',bordercolor='#888888',borderwidth=1,\n",
    "        font=dict(family='sans-serif',size=12,color='#000'),\n",
    "    )    \n",
    "    layout = go.Layout(margin=margin, legend=legend, title=title,\n",
    "        #showlegend=True,\n",
    "        xaxis=dict(\n",
    "            type='date',\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            type='linear',\n",
    "            range=[1, 100],\n",
    "            dtick=20,\n",
    "            ticksuffix='%'\n",
    "        )\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(fig, filename='stacked-area-plot')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T21:10:29.869584Z",
     "start_time": "2018-03-15T21:10:29.602466Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data processing\n",
    "\n",
    "def _start(s):\n",
    "    return s.index[0]\n",
    "\n",
    "def _end(s):\n",
    "    return s.index[-1]\n",
    "\n",
    "# def getCommonDate(data, alldata=False):\n",
    "#     if alldata:\n",
    "#         l = [_start(s) for s in data if isinstance(s, Wrapper) or isinstance(s, pd.Series)]\n",
    "#     else:\n",
    "#         l = [_start(s) for s in data if isinstance(s, Wrapper)]\n",
    "#     if not l:\n",
    "#         return None\n",
    "#     return max(l)\n",
    "\n",
    "def getCommonDate(data, alldata=False, agg=max, get_fault=False):\n",
    "    if alldata:\n",
    "        data = [s for s in data if isinstance(s, Wrapper) or isinstance(s, pd.Series)]\n",
    "    else:\n",
    "        data = [s for s in data if isinstance(s, Wrapper)]\n",
    "    if not data:\n",
    "        return None\n",
    "    dates = [_start(s) for s in data]\n",
    "    val = agg(dates)\n",
    "    if get_fault:\n",
    "        fault = \", \".join(s.name for date, s in zip(dates, data) if date == val)\n",
    "        return val, fault\n",
    "    return val\n",
    "\n",
    "def doTrim(data, alldata=False, silent=False):\n",
    "    if silent:\n",
    "        date = getCommonDate(data, alldata=alldata)\n",
    "    else:\n",
    "        date, max_fault = getCommonDate(data, alldata=alldata, get_fault=True)\n",
    "    if date is None:\n",
    "        if not silent:\n",
    "            print(\"Unable to trim data\")\n",
    "        return data\n",
    "    newArr = []\n",
    "    for s in data:\n",
    "        if isinstance(s, Wrapper) or (alldata and isinstance(s, pd.Series)):\n",
    "            s = s[date:]\n",
    "            if s.shape[0] == 0:\n",
    "                continue\n",
    "        newArr.append(s)\n",
    "    if not silent:\n",
    "        min_date, min_fault = getCommonDate(data, agg=min, get_fault=True)\n",
    "        print(f\"trimmed data from {min_date:%Y-%m-%d} [{min_fault}] to {date:%Y-%m-%d} [{max_fault}]\")\n",
    "    return newArr\n",
    "\n",
    "def trimBy(trimmed, by):\n",
    "    if len(by) == 0:\n",
    "        return []\n",
    "    start = max(s.index[0] for s in by)\n",
    "    return [s[start:] for s in trimmed]\n",
    "\n",
    "def doAlign(data):\n",
    "    date = getCommonDate(data)\n",
    "    if date is None:\n",
    "        return data\n",
    "    newArr = []\n",
    "    for s in data:\n",
    "        if isinstance(s, Wrapper) or isinstance(s, pd.Series):\n",
    "            #s = s / s[date] # this can sometime fail for messing data were not all series have the same index\n",
    "            base = s[date:]\n",
    "            if len(base.s) == 0:\n",
    "                continue\n",
    "            s = s / base[0]\n",
    "        newArr.append(s)\n",
    "    return newArr\n",
    "\n",
    "def doClean(data):\n",
    "    return [s.dropna() if isinstance(s, Wrapper) else s for s in data]\n",
    "\n",
    "def try_parse_date(s, format):\n",
    "    try:\n",
    "        return datetime.datetime.strptime(s, format)\n",
    "    except ValueError:\n",
    "        return None    \n",
    "\n",
    "def easy_try_parse_date(s):\n",
    "    return try_parse_date(s, \"%d/%m/%Y\") or try_parse_date(s, \"%d.%m.%Y\") or try_parse_date(s, \"%d-%m-%Y\")\n",
    "    \n",
    "def show(*data, trim=True, align=True, ta=True, cache=None, mode=None, source=None, silent=False, **plotArgs):\n",
    "    items = []\n",
    "    getArgs = {}\n",
    "    if not mode is None:\n",
    "        getArgs[\"mode\"] = mode\n",
    "    if not cache is None:\n",
    "        getArgs[\"cache\"] = cache\n",
    "    if not source is None:\n",
    "        getArgs[\"source\"] = cache\n",
    "    \n",
    "    data2 = []\n",
    "    for x in data:\n",
    "        if isinstance(x, list):\n",
    "            data2 += x\n",
    "        else:\n",
    "            data2.append(x)\n",
    "    \n",
    "    for x in data2:\n",
    "        if isinstance(x, pd.DataFrame):\n",
    "            items += [x[c] for c in x]\n",
    "        elif isinstance(x, datetime.datetime) or isinstance(x, np.datetime64):\n",
    "            items.append(x)\n",
    "        elif isinstance(x, str) and easy_try_parse_date(x):\n",
    "            items.append(easy_try_parse_date(x))\n",
    "        elif isinstance(x, numbers.Real):\n",
    "            items.append(x)\n",
    "        else:\n",
    "            x = get(x, **getArgs)\n",
    "            items.append(x)\n",
    "    data = items\n",
    "    #data = [get(s) for s in data] # converts string to symbols\n",
    "    data = doClean(data)\n",
    "    if not ta:\n",
    "        trim = False\n",
    "        align = False\n",
    "    if trim: data = doTrim(data)\n",
    "    if align: data = doAlign(data)\n",
    "        \n",
    "    if not silent:\n",
    "        plot(*data, **plotArgs)\n",
    "    else:\n",
    "        return [d for d in data if not isinstance(d, numbers.Real)]\n",
    "\n",
    "\n",
    "    \n",
    "def showRiskReturn(lst, setlim=True, lines=False, color=None, annotations=None, ret_func=None, risk_func=None):\n",
    "    if len(lst) == 0:\n",
    "        return\n",
    "    if ret_func is None:\n",
    "        ret_func = cagr\n",
    "    if risk_func is None:\n",
    "        risk_func = ulcer\n",
    "    lst = [get(s) for s in lst]\n",
    "    if annotations is None:\n",
    "        if \"name\" in dir(lst[0]) or \"s\" in dir(lst[0]):\n",
    "            annotations = [s.name for s in lst]\n",
    "    cagrs = [ret_func(s) for s in lst]\n",
    "    stds = [risk_func(s) for s in lst]\n",
    "    #stds = [stdmret(s) for s in lst]\n",
    "    if lines:\n",
    "        plt.plot(stds, cagrs, marker=\"o\", color=color)\n",
    "    else:\n",
    "        plt.scatter(stds, cagrs, color=color)\n",
    "    if setlim:\n",
    "        plt.xlim(min(0, min(stds)-0), max(stds)+1)\n",
    "        plt.ylim(min(0, min(cagrs)-1), max(cagrs)+1)\n",
    "    plt.axhline(0, color='gray', linewidth=1)\n",
    "    plt.axvline(0, color='gray', linewidth=1)\n",
    "    plt.xlabel(risk_func.__name__, fontsize=20)\n",
    "    plt.ylabel(ret_func.__name__, fontsize=20)\n",
    "    if annotations:\n",
    "        for i, txt in enumerate(annotations):\n",
    "            plt.annotate(txt, (stds[i], cagrs[i]), fontsize=14)\n",
    "\n",
    "def show_risk_return_ntr_mode(lst, ret_func=None):\n",
    "    def get_data(lst, mode):\n",
    "        return get(lst, mode=mode, despike=True, trim=True)\n",
    "\n",
    "    tr = get_data(lst, \"TR\")\n",
    "    ntr = get_data(lst, \"NTR\")\n",
    "    showRiskReturn(ntr, ret_func=ret_func)\n",
    "    for a, b in zip(tr, ntr):\n",
    "        showRiskReturn([a, b], setlim=False, lines=True, ret_func=ret_func, annotations=False)    \n",
    "\n",
    "def mix(s1, s2, n=10, **getArgs):\n",
    "    part = 100/n\n",
    "    res = []\n",
    "    for i in range(n+1):\n",
    "        res.append(get({s1: i*part, s2: (100-i*part)}, **getArgs))\n",
    "    return res\n",
    "        \n",
    "def ma(s, n):\n",
    "    n = int(n)\n",
    "    return wrap(s.rolling(n).mean(), \"ma({}, {})\".format(s.name, n))\n",
    "\n",
    "def mm(s, n):\n",
    "    n = int(n)\n",
    "    return wrap(s.rolling(n).median(), \"mm({}, {})\".format(s.name, n))\n",
    "\n",
    "def mmax(s, n):\n",
    "    n = int(n)\n",
    "    return wrap(s.rolling(n).max(), \"mmax({}, {})\".format(s.name, n))\n",
    "\n",
    "def mmin(s, n):\n",
    "    n = int(n)\n",
    "    return wrap(s.rolling(n).min(), \"mmin({}, {})\".format(s.name, n))\n",
    "\n",
    "def cagr(s):\n",
    "    days = (s.index[-1] - s.index[0]).days\n",
    "    if days <= 0:\n",
    "        return np.nan\n",
    "    years = days/365\n",
    "    val = s[-1] / s[0]\n",
    "    return (math.pow(val, 1/years)-1)*100\n",
    "\n",
    "def ulcer(x):\n",
    "    cmax = np.maximum.accumulate(x)\n",
    "    r = (x/cmax-1)*100\n",
    "    return math.sqrt(np.sum(r*r)/x.shape[0])\n",
    "\n",
    "# std of monthly returns\n",
    "def stdmret(s):\n",
    "    return ret(s).std()*math.sqrt(12)*100\n",
    "\n",
    "def bom(s):\n",
    "    idx = s.index.values.astype('datetime64[M]') # convert to monthly representation\n",
    "    idx = np.unique(idx) # remove duplicates\n",
    "    return s[idx].dropna()\n",
    "    \n",
    "def boy(s):\n",
    "    idx = s.index.values.astype('datetime64[Y]') # convert to monthly representation\n",
    "    idx = np.unique(idx) # remove duplicates\n",
    "    return s[idx].dropna()\n",
    "    \n",
    "def ret(s):\n",
    "    return s.pct_change()\n",
    "\n",
    "def logret(s):\n",
    "    res = np.log(s) - np.log(s.shift(1))\n",
    "    res.name = \"logret(\" + s.name + \")\"\n",
    "    return res\n",
    "\n",
    "def i_logret(s):\n",
    "    return np.exp(np.cumsum(s))\n",
    "\n",
    "def lrret(regressors, target, sum1=False):\n",
    "    regressors = [get(x) for x in regressors]\n",
    "    target = get(target)\n",
    "    all = [logret(x).s for x in (regressors + [target])]\n",
    "    \n",
    "    # based on: https://stats.stackexchange.com/questions/21565/how-do-i-fit-a-constrained-regression-in-r-so-that-coefficients-total-1?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "    # NOTE: note finished, not working\n",
    "    if sum1:\n",
    "        allOrig = all\n",
    "        last = all[-2]\n",
    "        all = [r - last for r in (all[:-2] + [all[-1]])]\n",
    "        \n",
    "    data = pd.DataFrame(all).T\n",
    "    data = data.dropna()\n",
    "    y = data.iloc[:, -1]\n",
    "    X = data.iloc[:, :-1]\n",
    "\n",
    "    regr = linear_model.LinearRegression(fit_intercept=False)\n",
    "    regr.fit(X, y)\n",
    "    \n",
    "    if sum1:\n",
    "        weights = np.append(regr.coef_, 1-np.sum(regr.coef_))\n",
    "        \n",
    "        all = allOrig\n",
    "        data = pd.DataFrame(all).T\n",
    "        data = data.dropna()\n",
    "        y = data.iloc[:, -1]\n",
    "        X = data.iloc[:, :-1]\n",
    "        regr = linear_model.LinearRegression(fit_intercept=False)\n",
    "        regr.fit(X, y)\n",
    "        \n",
    "        regr.coef_ = weights\n",
    "    \n",
    "    y_pred = regr.predict(X)\n",
    "\n",
    "    \n",
    "    print('Regressors:', [s.name for s in regressors])\n",
    "    print('Coefficients:', regr.coef_)\n",
    "    #print('Coefficients*:', list(regr.coef_) + [1-np.sum(regr.coef_)])\n",
    "    #print(\"Mean squared error: %.2f\" % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "    print('Variance score r^2: %.3f' % sk.metrics.r2_score(y, y_pred))\n",
    "\n",
    "    y_pred = i_logret(pd.Series(y_pred, X.index))\n",
    "    y_pred.name = target.name + \" fit\"\n",
    "    #y_pred = \"fit\"\n",
    "    y_pred = Wrapper(y_pred)\n",
    "    show(target , y_pred)\n",
    "    return y_pred\n",
    "    \n",
    "def dd(x):\n",
    "    if isinstance(x, Wrapper): # not sure why Wrapper doesn't work\n",
    "        x = x.s\n",
    "    x = x.dropna()\n",
    "    res = (x / np.maximum.accumulate(x) - 1) * 100\n",
    "    return res\n",
    "    \n",
    "def percentile(s, p):\n",
    "    return s.quantile(p/100)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T20:31:50.999302Z",
     "start_time": "2018-03-15T20:31:50.989870Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import Javascript\n",
    "import time, os, stat\n",
    "\n",
    "def save_notebook(verbose=True, sleep=True):\n",
    "    Javascript('console.log(document.querySelector(\"div#save-notbook button\").click())')\n",
    "    if verbose:\n",
    "        print(\"save requested, sleeping to ensure execution ..\")\n",
    "    if sleep:\n",
    "        time.sleep(15)\n",
    "    if verbose:\n",
    "        print(\"done\")\n",
    "\n",
    "# save live notebook at first run to make sure it's the latest modified file in the folder (for later publishing)\n",
    "save_notebook(False, False)\n",
    "\n",
    "def publish(name=None):\n",
    "    def file_age_in_seconds(pathname):\n",
    "        return time.time() - os.stat(pathname)[stat.ST_MTIME]\n",
    "\n",
    "    filename = !ls -t *.ipynb | grep -v /$ | head -1\n",
    "    filename = filename[0]\n",
    "\n",
    "    age = int(file_age_in_seconds(filename))\n",
    "    min_age = 5\n",
    "    if age > min_age:\n",
    "        print(filename + \" file age is \" + str(age) + \" seconds old, auto saving current notebook ..\")\n",
    "        save_notebook()\n",
    "        filename = !ls -t *.ipynb | grep -v /$ | head -1\n",
    "        filename = filename[0]\n",
    "    \n",
    "    if not name:\n",
    "        name = str(uuid.uuid4().hex.upper())\n",
    "    save()\n",
    "    print(\"Publishing \" + filename + \" ..\")\n",
    "    res = subprocess.call(['bash', './publish.sh', name])\n",
    "    if res == 0:\n",
    "        print(\"published successfuly!\")\n",
    "        print(\"https://nbviewer.jupyter.org/github/ertpload/test/blob/master/__name__.ipynb\".replace(\"__name__\", name))\n",
    "    else:\n",
    "        print(\"Failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T20:31:51.004297Z",
     "start_time": "2018-03-15T20:31:51.001149Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display,Javascript \n",
    "def save():\n",
    "    display(Javascript('IPython.notebook.save_checkpoint();'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T20:31:51.012037Z",
     "start_time": "2018-03-15T20:31:51.006524Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make the plotly graphs look wider on mobile\n",
    "from IPython.core.display import display, HTML\n",
    "s = \"\"\"\n",
    "<style>\n",
    "div.rendered_html {\n",
    "    max-width: 10000px;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "display(HTML(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T20:31:51.050038Z",
     "start_time": "2018-03-15T20:31:51.014638Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# interception to auto-fetch hardcoded symbols e.g:\n",
    "# show(SPY)\n",
    "# this should run last in the framework code, or it attempts to download unrelated symbols :)\n",
    "\n",
    "from IPython.core.inputtransformer import *\n",
    "intercept = True\n",
    "if intercept and not \"my_transformer_tokens_instance\" in locals():\n",
    "    #print(\"transformation hook init\")\n",
    "    attempted_implied_fetches = set()\n",
    "    \n",
    "    ip = get_ipython()\n",
    "\n",
    "    @StatelessInputTransformer.wrap\n",
    "    def my_transformer(line):\n",
    "        if line.startswith(\"x\"):\n",
    "            return \"specialcommand(\" + repr(line) + \")\"\n",
    "        return line\n",
    "\n",
    "    @TokenInputTransformer.wrap\n",
    "    def my_transformer_tokens(tokens):\n",
    "        for i, x in enumerate(tokens):\n",
    "            if x.type == 1 and x.string.isupper() and x.string.isalpha(): ## type=1 is NAME token\n",
    "                if i < len(tokens)-1 and tokens[i+1].type == 53 and tokens[i+1].string == \"=\":\n",
    "                    attempted_implied_fetches.add(x.string)\n",
    "                    continue\n",
    "                if x.string in attempted_implied_fetches or x.string in ip.user_ns:\n",
    "                    continue\n",
    "                try:\n",
    "                    ip.user_ns[x.string] = get(x.string)\n",
    "                except:\n",
    "                    print(\"Failed to fetch implied symbol: \" + x.string)\n",
    "                    attempted_implied_fetches.add(x.string)\n",
    "        return tokens\n",
    "\n",
    "    my_transformer_tokens_instance = my_transformer_tokens()\n",
    "    \n",
    "    ip.input_splitter.logical_line_transforms.append(my_transformer_tokens_instance)\n",
    "    ip.input_transformer_manager.logical_line_transforms.append(my_transformer_tokens_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T20:31:51.056535Z",
     "start_time": "2018-03-15T20:31:51.052816Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date(s):\n",
    "    return pd.to_datetime(s, format=\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ************* SYMBOLS ***************\n",
    "# these are shorthand variables representing asset classes\n",
    "\n",
    "# ==== SPECIAL ====\n",
    "# https://www.federalreserve.gov/pubs/bulletin/2005/winter05_index.pdf\n",
    "# Nominal Daily\n",
    "usdMajor = 'FRED/DTWEXM@Q' # Trade Weighted U.S. Dollar Index: Major Currencies\n",
    "usdBroad = 'FRED/DTWEXB@Q' # Trade Weighted U.S. Dollar Index: Broad\n",
    "usdOther = 'FRED/DTWEXO@Q' # Trade Weighted U.S. Dollar Index: Other Important Trading Partners\n",
    "# Nominal Monthly\n",
    "usdMajorM = 'FRED/TWEXMMTH@Q'\n",
    "usdBroadM = 'FRED/TWEXBMTH@Q'\n",
    "usdOtherM = 'FRED/TWEXOMTH@Q'\n",
    "# Real Monthly\n",
    "usdMajorReal = 'FRED/TWEXMPA@Q' # Real Trade Weighted U.S. Dollar Index: Major Currencies\n",
    "usdBroadReal = 'FRED/TWEXBPA@Q' # Real Trade Weighted U.S. Dollar Index: Broad\n",
    "usdOtherReal = 'FRED/TWEXOPA@Q' # Real Trade Weighted U.S. Dollar Index: Other Important Trading Partners\n",
    "usd = usdBroad\n",
    "\n",
    "cpiUS ='RATEINF/CPI_USA@Q'\n",
    "\n",
    "\n",
    "#bitcoinAvg = price(\"BAVERAGE/USD@Q\") # data 2010-2016\n",
    "#bitcoinBitstamp = price(\"BCHARTS/BITSTAMPUSD@Q\") # data 2011-now\n",
    "\n",
    "# ==== STOCKS ====\n",
    "# Global\n",
    "g_ac = 'VTSMX:45|VGTSX:55' # VT # global all-cap\n",
    "d_ac = 'URTH' # developed world\n",
    "# US\n",
    "ac = 'VTSMX' # VTI # all-cap\n",
    "lc = 'VFINX' # VOO, SPY # large-cap\n",
    "mc = 'VIMSX' # VO # mid-cap\n",
    "sc = 'NAESX' # VB # small-cap\n",
    "mcc = 'BRSIX' # micro-cap\n",
    "lcv = 'VIVAX' # IUSV # large-cap-value\n",
    "mcv = 'VMVIX' # mid-cap-value\n",
    "scv = 'VISVX' # VBR # small-cap-value\n",
    "lcg = 'VIGRX' # large-cap-growth \n",
    "mcg = 'VMGIX' # mid-cap-growth\n",
    "scg = 'VISGX' # VBK # small-cap-growth\n",
    "# ex-US\n",
    "i_ac = 'VGTSX' # VXUS # intl' all-cap\n",
    "i_sc = 'VINEX' # VSS, SCZ # intl' small-cap\n",
    "d_ac = 'VTMGX' # EFA, VEA # intl' developed\n",
    "i_dev = d_ac # legacy\n",
    "i_acv = 'DFIVX' # EFV # intl' all-cap-value\n",
    "i_scv = 'DISVX' # DLS # intl' small-cap-value\n",
    "em_ac = 'VEIEX' # VWO # emerging markets\n",
    "em = em_ac # legacy\n",
    "em_sc = 'EEMS' # emerging markets small cap\n",
    "fr_ac = 'FRN' # FM # frontier markets\n",
    "\n",
    "# ==== BONDS ====\n",
    "# US GOVT\n",
    "sgb = 'VFISX' # SHY, VGSH # short term govt bonds\n",
    "tips = 'VIPSX' # TIP # inflation protected treasuries\n",
    "lgb = 'VUSTX' # TLT, VGLT # long govt bonds\n",
    "elgb = 'PEDIX' # EDV # extra-long (extended duration) govt bonds\n",
    "gb = 'VFITX' # IEI # intermediate govt bonds\n",
    "fgb = 'TFLO' # floating govt bonds\n",
    "# US CORP \n",
    "cb = 'MFBFX' # LQD # corp bonds\n",
    "scb = 'VCSH' # short-term-corp-bonds\n",
    "lcb = 'VCLT' # long-term-corp-bonds\n",
    "fcb = 'FLOT' # floating corp bonds\n",
    "# US CORP+GOVT\n",
    "gcb = 'VBMFX' # AGG, BND # govt/corp bonds\n",
    "sgcb = 'VFSTX' # BSV # short-term-govt-corp-bonds\n",
    "# International\n",
    "i_tips = 'WIP' # # intl' local currency inflation protected bonds\n",
    "i_gcbUsd = 'PFORX' # BNDX # ex-US govt/copr bonds (USD hedged)\n",
    "i_gbLcl = 'BEGBX' # (getBwx()) BWX, IGOV # ex-US govt bonds (non hedged)\n",
    "i_gb = i_gbLcl # legacy\n",
    "i_cb = 'PIGLX' # PICB, ex-US corp bonds\n",
    "i_cjb = 'IHY' # intl-corp-junk-bonds\n",
    "g_gcbLcl = 'PIGLX' # Global bonds (non hedged)\n",
    "g_gcbUsd = 'PGBIX' # Global bonds (USD hedged)\n",
    "g_sgcb = 'LDUR' # Global short-term govt-corp bonds\n",
    "g_usgcb = 'MINT' # Global ultra-short-term govt-corp bonds\n",
    "em_gbUsd = 'FNMIX' # VWOB, EMB # emerging market govt bonds (USD hedged)\n",
    "emb = em_gbUsd # legacy\n",
    "em_gbLcl = 'PELBX' # LEMB, EBND, EMLC emerging-markets-govt-bonds (local currency) [LEMB Yahoo data is broken]\n",
    "em_cjb = 'EMHY' # emerging-markets-corp-junk-bonds\n",
    "cjb = 'VWEHX' # JNK, HYG # junk bonds\n",
    "junk = 'cjb' # legacy\n",
    "scjb = 'HYS' # short-term-corp-junk-bonds\n",
    "\n",
    "# ==== CASH ====\n",
    "rfr = 'SHV' # BIL # risk free return (1-3 month t-bills)\n",
    "cash = 'rfr' # SHV # risk free return\n",
    "cashLike = 'VFISX:30' # a poor approximation for rfr returns \n",
    "\n",
    "# ==== OTHER ====\n",
    "fedRate = 'FRED/DFF@Q'\n",
    "reit = 'DFREX' # VNQ # REIT\n",
    "i_reit = 'RWX' # VNQI # ex-US REIT\n",
    "g_reit = 'DFREX:50|RWX:50' # RWO # global REIT\n",
    "gold = 'LBMA/GOLD@Q' # GLD # gold\n",
    "silver = 'LBMA/SILVER@Q' # SLV # silver\n",
    "palladium = 'LPPM/PALL@Q'\n",
    "platinum = 'LPPM/PLAT@Q'\n",
    "#metals = gold|silver|palladium|platinum # GLTR # precious metals (VGPMX is a stocks fund)\n",
    "comm = 'DBC' # # commodities\n",
    "oilWtiQ = 'FRED/DCOILWTICO@Q'\n",
    "oilBrentQ = 'FRED/DCOILBRENTEU@Q'\n",
    "oilBrentK = 'oil-prices@OKFN' # only loads first series which is brent\n",
    "eden = 'EdenAlpha@MAN'\n",
    "\n",
    "# ==== INDICES ====\n",
    "spxPR = '^GSPC'\n",
    "spxTR = '^SP500TR'\n",
    "spx = spxPR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another options for interception:\n",
    "```python\n",
    "class VarWatcher(object):\n",
    "    def __init__(self, ip):\n",
    "        self.shell = ip\n",
    "        self.last_x = None\n",
    "\n",
    "    def pre_execute(self):\n",
    "        if False:\n",
    "            for k in dir(self.shell):\n",
    "                print(k, \":\", getattr(self.shell, k))\n",
    "                print()\n",
    "        #print(\"\\n\".join(dir(self.shell)))\n",
    "        if \"content\" in self.shell.parent_header:\n",
    "            code = self.shell.parent_header['content']['code']\n",
    "            self.shell.user_ns[code] = 42\n",
    "        #print(self.shell.user_ns.get('ASDF', None))\n",
    "\n",
    "    def post_execute(self):\n",
    "        pass\n",
    "        #if self.shell.user_ns.get('x', None) != self.last_x:\n",
    "        #    print(\"x changed!\")\n",
    "\n",
    "def load_ipython_extension(ip):\n",
    "    vw = VarWatcher(ip)\n",
    "    ip.events.register('pre_execute', vw.pre_execute)\n",
    "    ip.events.register('post_execute', vw.post_execute)\n",
    "    \n",
    "ip = get_ipython()\n",
    "\n",
    "load_ipython_extension(ip)   \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divs(symbolName, period=None, fill=False):\n",
    "    if isinstance(symbolName, tuple) and period is None:\n",
    "        symbolName, period = symbolName\n",
    "    if isinstance(symbolName, Wrapper) or isinstance(symbolName, pd.Series):\n",
    "        symbolName = symbolName.name\n",
    "    divs = get(symbolName, mode=\"divs\")\n",
    "    divs = divs[divs.s>0]\n",
    "    if period:\n",
    "        divs = wrap(divs.rolling(period).sum())\n",
    "    if fill:\n",
    "        price = get(symbolName)\n",
    "        divs = divs.reindex(price.index.union(divs.index), fill_value=0)        \n",
    "    return divs\n",
    "\n",
    "def getYield(symbolName, period=None, altPriceName=None):\n",
    "    if isinstance(symbolName, tuple) and period is None:\n",
    "        symbolName, period = symbolName\n",
    "    if isinstance(symbolName, Wrapper) or isinstance(symbolName, pd.Series):\n",
    "        symbolName = symbolName.name\n",
    "    price = get(altPriceName or symbolName, mode=\"PR\")\n",
    "    divs = get(symbolName, mode=\"divs\")\n",
    "    divs = divs[divs.s>0]\n",
    "    if len(divs.s) == 0:\n",
    "        return divs\n",
    "    if period is None:\n",
    "        monthds_diff = (divs.s.index.to_series().diff().dt.days/30).dropna().apply(lambda x: int(round(x)))\n",
    "        months = monthds_diff[-5:].median()\n",
    "        period = int(12 // months)\n",
    "\n",
    "        #periods = divs.s.index.year.value_counts()\n",
    "        #periods = periods.sort_index()\n",
    "        #periods = periods[-5:]\n",
    "        #period = int(periods.median())\n",
    "        \n",
    "        #print(f\"auto period for {symbolName} is {period}\")\n",
    "        #print(divs.s.index.year.value_counts())\n",
    "    if period:\n",
    "        divs = wrap(divs.rolling(period).sum())\n",
    "    return name(divs/price*100, divs.name)\n",
    "\n",
    "def get_curr_yield(s, period=None):\n",
    "    return getYield(s, period=period).dropna()[-1]\n",
    "\n",
    "def get_curr_net_yield(s, period=None):\n",
    "    return getYield(s, period=period).dropna()[-1]*0.75\n",
    "\n",
    "def get_TR_from_PR_and_divs(pr, divs):\n",
    "    m = d / pr + 1\n",
    "    mCP = m.cumprod().fillna(method=\"ffill\")\n",
    "    tr = pr * mCP\n",
    "    return wrap(tr, pr.name + \" TR\")\n",
    "\n",
    "def despike(s, std=8, window=30, shift=10):\n",
    "    if isinstance(s, list):\n",
    "        return [despike(x) for x in s]\n",
    "    if \"s\" in dir(s):\n",
    "        s = s.s\n",
    "    new_s = s.copy()\n",
    "    ret = logret(s).fillna(0)\n",
    "    new_s[(ret - ret.mean()).abs() > ret.shift(shift).rolling(window).std().fillna(ret.max()) * std] = np.nan\n",
    "    return wrap(new_s.interpolate(), s.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T19:18:24.355980Z",
     "start_time": "2018-07-21T19:18:24.350454Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# safely convert a float/string/mixed series to floats\n",
    "# to remove commas we need the data type to be \"str\"\n",
    "# but if we assume it's \"str\" wihtout converting first, and some are numbers\n",
    "# those numbers will become NaN's.\n",
    "def series_as_float(ser):\n",
    "    return pd.to_numeric(ser.astype(str).str.replace(\",\", \"\").str.replace(\"%\", \"\"), errors=\"coerce\")\n",
    "\n",
    "def lmap(f, l):\n",
    "    return list(map(f, l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
